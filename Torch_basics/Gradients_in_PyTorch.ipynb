{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "\n",
    "Gradients and optimization are essential parts in training neural networks. In PyTorch it is implemented in a quite convenient way. That is why, in this notebook I want to discuss AutoGrad and Backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 AutoGrad\n",
    "\n",
    "It is PyTorch's automatic differentiation engine. In previous notebook, we looked at the example with Gradient Descent. Here I want to discuss the topic in a more concrete way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Intro\n",
    "\n",
    "Suppose we have an initial vector x. We made some other vectors from it, such as y = x + 2, and $z = 2y^2$. As a result, we will be able to calculate the gradient of the final vector with respect to the initial one. In our case, remember the chain rule:\n",
    "\n",
    "$$\\frac{\\partial{z}}{\\partial{x}} = \\frac{\\partial{z}}{\\partial{y}} \\frac{\\partial{y}}{\\partial{x}} = 4y.$$\n",
    "\n",
    "By default, gradients are calculated for scalars. This is usually sufficient, as in neural networks we want to minimize MSE or something close to it, which are usually the average of individual losses. Let's see how it looks like in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.8894, -0.1854, -1.1037], requires_grad=True)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.8894, 1.8146, 0.8963], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x + 2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "z =  (2 * y ** 2).mean() # gradients are created only for scalars\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.8525, 2.4194, 1.1950])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.8525, 2.4194, 1.1950], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4 * y / 3\n",
    "# we see that indeed the gradient was calculated correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 One clarification\n",
    "\n",
    "Suppose we want to create a new vector from another vector (for example, to test something), but we do not need its gradients. In that case we can save time and computational resources by using torch.no_grad() decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5., 10., 15.], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.0,2.0,3.0], requires_grad=True)\n",
    "y = x * 5\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5., 10., 15.])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.0,2.0,3.0], requires_grad=True)\n",
    "with torch.no_grad():\n",
    "    y = x * 5\n",
    "y\n",
    "# as we see, we do not have gradients for y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Backpropagation\n",
    "\n",
    "Basically, Backpropagation is just the chain rule (I provided an example earlier). We use local gradients to compute the gradient of the loss with respect to our parameters. There are two steps:\n",
    "\n",
    "1) Do the forward pass. In this step, the neural network makes its best guess about the correct output, based on what it already learnt. We also calculate loss for its further minimization\n",
    "\n",
    "2) Backpropagation. Here we adjust the parameters based on our loss.\n",
    "\n",
    "Again, let's see how this is done on the simples possible example: $y = 2x.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "y = torch.tensor([2.0, 4.0, 6.0])\n",
    "m = torch.tensor(1.0, requires_grad=True) # our initial guess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.6667, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = m * x\n",
    "loss = ((y_pred - y) ** 2).mean() # calculate MSE\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Backpropagation and parameter update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-9.3333)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.backward() # calculate gradient\n",
    "m.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0933, grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = .01\n",
    "m = m - learning_rate * m.grad\n",
    "m\n",
    "# as we see, we move in the right direction, since the slope increased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1) https://www.youtube.com/watch?v=c36lUUr864M&t=2181s - a great PyTorch course for beginners\n",
    "\n",
    "2) https://pytorch.org/docs/stable/index.html - PyTorch documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
