{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File summary\n",
    "\n",
    "In PyTorch everything is done using tensor operations. Basically, tensors are similar to NumPy arrays, which we are all familiar with. However, there are few differences:\n",
    "\n",
    "1) Tensors in PyTorch can do automatic differentiation, while in NumPy we have to write gradients by ourselves, or use some other tools to do so\n",
    "\n",
    "2) Tensors are supported by GPU. So, in case of complicated and big calculations, tensors might be more preferable\n",
    "\n",
    "3) Hence, tensors are more used in Deep Learning. However, NumPy is sufficient in many classical Machine Learning problems, while still being simple and well-known tool. Therefore, in Machine Learning problems NumPy is probably more popular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2.], requires_grad=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch tensor is like np.array\n",
    "torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "# requires_grad means that we are going to use this tensor to compute gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an empty tensor of size 3 x 2\n",
    "torch.empty(3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a tensor of ones\n",
    "torch.ones(4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3492, 0.3032, 0.5313],\n",
       "        [0.3499, 0.4998, 0.5187]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a tensor with random values of size 2 x 3\n",
    "torch.rand(2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Operations with tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "\n",
      "tensor([[5, 6],\n",
      "        [7, 8]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1,2], [3,4]])\n",
    "print(a)\n",
    "print()\n",
    "b = torch.tensor([[5,6], [7,8]])\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6,  8],\n",
       "        [10, 12]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5, 12],\n",
       "        [21, 32]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# element-wise multiplication\n",
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19, 22],\n",
       "        [43, 50]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrix multiplication\n",
    "a @ b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Some additional things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert tensor to numpy array\n",
    "b = torch.tensor([[5,6], [7,8]])\n",
    "b = b.numpy()\n",
    "type(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 3],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "# convert numpy array to tensor\n",
    "a = np.array([[1,2], [3,4]])\n",
    "b = torch.from_numpy(a)\n",
    "a += 1\n",
    "print(b)\n",
    "# note that when we change initial array, the tensor changes as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Gradient Descent \n",
    "\n",
    "Here I want to show the difference between implementing Gradient Descent algorithm in PyTorch and in NumPy. We will do it on the simplest case. Here I will not explain what is Gradient Descent. For more information, please check one of my previous projects (https://github.com/AlgazinovAleksandr/My_projects/blob/main/Convex_optimization.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Example\n",
    "\n",
    "Suppose we have a vector x = [1,2,3,4,5,6] and a vector of y, where $y_i = 3x_i + 7$. So, in general $y = mx + b$, and we will try to find parameters m and b using Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,2,3,4,5,6])\n",
    "y = 3 * x + 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 NumPy\n",
    "\n",
    "We will start with writing a classical Gradient Descent implementation. Note that we do not even use NumPy to make it work. Here NumPy is used just to create the arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_numpy(x, y, num_epochs=5000, learning_rate=.01):\n",
    "    \n",
    "    m = 1\n",
    "    b = 1\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        \n",
    "        y_pred = m * x + b\n",
    "        grad_m = np.mean(2 * (y_pred - y) * x)\n",
    "        grad_b = np.mean(2 * (y_pred - y))\n",
    "        m -= learning_rate * grad_m\n",
    "        b -= learning_rate * grad_b\n",
    "            \n",
    "    return m, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.000000014098132, 6.999999939643098)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m, b = gradient_descent_numpy(x, y)\n",
    "m, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything seems simple besides the fact that we need to know the gradients. Sometimes, it might be complicated or at least take some time to calculate them. In such cases torch becomes very useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 PyTorch\n",
    "\n",
    "We already know that if we set requires_grad=True, gradients can be calculated automatically if we ask to do so. This makes the process simpler. Note that we will talk about the differentiation in PyTorch in the following jupyter notebooks, and here I just want to show an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1,2,3,4,5,6])\n",
    "y = 3 * x + 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_torch(x, y, num_epochs=5000, learning_rate=.01):\n",
    "    \n",
    "    m = torch.tensor(1.0, requires_grad=True)\n",
    "    b = torch.tensor(1.0, requires_grad=True)\n",
    "    optimizer = torch.optim.SGD([m, b], lr=learning_rate)\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "    \n",
    "        y_pred = m * x + b\n",
    "        loss = torch.mean((y_pred - y)**2)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    return m, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.0000157356262207, 6.999932765960693)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m, b = gradient_descent_torch(x, y)\n",
    "float(m), float(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, we did not specify the gradients on our own and still managed to get the same answer as we did before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1) https://www.youtube.com/watch?v=c36lUUr864M&t=2181s - a great PyTorch course for beginners\n",
    "\n",
    "2) https://github.com/AlgazinovAleksandr/My_projects/blob/main/Convex_optimization.ipynb - my project on Convex Optimization\n",
    "\n",
    "3) https://pytorch.org/docs/stable/index.html - PyTorch documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
